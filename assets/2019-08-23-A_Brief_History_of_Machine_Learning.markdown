---
title: A Brief History of Machine Learning
author: Jaideep Ganguly
---

{% include header.html %}

The buzz around Machine Learning and Deep Learning prompted me to trace the history of Artificial Intelligence at [MIT](http://csail.mit.edu) and elsewhere and take stock of the current state of Machine Learning. [Arthur Samuel](http://en.wikipedia.org/wiki/Arthur_Samuel), who is best known for the checkers playing program, is credited with coining the name "Machine Learning". Before we get started, a quick overview of some terms; Learning is acquisition of knowledge, discovery is the observation of a new phenomena and invention is the process of making something new. Learning is necessary for invention but is not a sufficient condition for innovation. Machine Learning as it stands today, does not invent but it does discover patterns in large quantities of data. In particular, Deep Neural Networks, have attracted the imagination of many because of some interesting solutions it offers in all three channels - text, speech and images. Incidentally, most deep neural nets are rather wide and are usually not more than ten layers deep. So the name should have really been "wide neural nets" but the word "deep" has stuck. 

"The question of whether a computer can think is no more interesting than the question of whether a submarine can swim", said [Dijkstra](http://en.wikipedia.org/wiki/Edsger_W._Dijkstra). It is more interesting to understand the evolution of Machine Learning - how did it start, where are we today and where do we go from here. The human brain is a remarkable thing; it has enabled us understand science and advance mankind. The idea of mimicking the human brain or even improving the human cognitive functions is an alluring one and is an objective of Artificial Intelligence research. But we are not even close in-spite of a century of research. However, it continues to have a major hold on our imagination given the potential of the rewards.

It seems that about 50,000 years ago, after we have been around for about a hundred thousand years or so, palaeontologists believe that some of us, possibly just a few thousand, were able to deal with symbols. This is a major step in [evolution](http://en.wikipedia.org/wiki/Timeline_of_human_evolution). [Noam Chomsky](http://en.wikipedia.org/wiki/Noam_Chomsky) thinks we were then able to create a new concept from 2 existing ideas or concepts without damaging or limiting the existing concepts. Around 350 BC, [Aristotle](http://en.wikipedia.org/wiki/Aristotle) devised [syllogistic logic](http://en.wikipedia.org/wiki/Syllogism), the first formal deductive reasoning system to model the way humans think about their world and reason with it. 2,000 years later, [Bertrand Russel](http://en.wikipedia.org/wiki/Bertrand_Russell) and [Alfred Whitehead](http://en.wikipedia.org/wiki/Alfred_North_Whitehead) published [Principia Mathematica](http://en.wikipedia.org/wiki/Principia_Mathematica) that laid down the foundations for a formal representation of Mathematics. [John McCarthy](http://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)), who championed the cause of mathematical logic in AI, was Aristotle of his day. In 1942, [Alan Turing](http://en.wikipedia.org/wiki/Alan_Turing) showed that any form of mathematical reasoning could be processed by a machine. By 1967, [Marvin Minsky](http://www.linkedin.com/post/edit/6199460470462701568) declared that "within a generation, the problem of creating Artificial Intelligence would substantially be solved". Clearly we are not there yet, attempts to build systems with first order logic as described by early philosophers failed because of lack of computing power, inability to deal with uncertainty and lack of large amounts of data. 

In 1961, Minsky published ["Steps towards Artificial Intelligence"](http://web.media.mit.edu/~minsky/papers/steps.html) where he talked about search, matching, probability, learning and it was quite visionary. Turing told us that it was possible to make a machine intelligent and Minsky told us how. In 1986, Minsky wrote the highly influential book ["The Society of Mind"](http://www.acad.bg/ebook/ml/Society%20of%20Mind.pdf), 24 centuries after Plato wrote "Politeia"; Minsky was [Plato](http://en.wikipedia.org/wiki/Plato) of his days. Minsky taught us to think about heuristic programming, McCarthy wanted us to use logic to the extreme, [Newel](http://en.wikipedia.org/wiki/Allen_Newell) wanted to build cognitive models of problem solving and [Simon](http://en.wikipedia.org/wiki/Herbert_A._Simon) believed that when we see something that is complicated in behavior, it is more of a consequence of a complex environment rather than because of a complex thinker. Thereafter, a number of model backed systems were built. [Terry Winograd](http://en.wikipedia.org/wiki/Terry_Winograd) built a model backed system for dialog understanding, Patrick Winston built another model backed system for learning and [Gerald Sussman](http://en.wikipedia.org/wiki/Gerald_Jay_Sussman) built a model backed system for understanding blocks. During the same era [Roger Schank](http://en.wikipedia.org/wiki/Roger_Schank) believed that understanding stories is the key to modeling human intelligence. [David Marr](http://en.wikipedia.org/wiki/David_Marr_(neuroscientist)), who is best known for his work on vision, treated vision as an information processing system. Marr's tri-level hypothesis in cognitive science comprised of a computational level - what does the system do, an algorithmic level - how does the system do and a physical level - how is the system physically realized, e.g., in the case of biological vision, what neural structures and neuronal activities implement the visual system. 

In the 1980s, the expert systems were of great interest and focused on knowledge and inference mechanisms. While these systems did a pretty good job in their domains, they were narrow in specialization and were difficult to scale. The field of AI was defined as computers performing tasks that were specifically thought of as something only humans can do. However, once these systems worked, they were no longer considered to be AI! For example, today the best chess players are routinely defeated by computers but chess playing is no longer really considered as AI! McCarthy referred to as the "AI effect". [IBM's](http://www.ibm.com/watson/) Watson is a program at a level such as that of a human expert but it is not certainly not the first one. Fifty years ago [Jim Slagle's symbolic integration program at MIT](http://dspace.mit.edu/bitstream/handle/1721.1/11997/31225400-MIT.pdf?sequence=2) was a tremendous achievement. Nevertheless, it is very hard to build a program that has "common sense" and not just narrow domains of knowledge. 

Today, at the core is the debate between logic inspired and neural network inspired paradigms for cognition. [LeCun, Bengio and Hinton state that succinctly in a review paper in Nature, dated 28th May 2015](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf), as "The issue of representation lies at the heart of the debate between the logic-inspired and the neural-network-inspired paradigms for cognition. In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbol instances. It has no internal structure that is relevant to its use; and to reason with symbols, they must be bound to the variables in judiciously chosen rules of inference. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning".

[Rosenblatt](http://en.wikipedia.org/wiki/Frank_Rosenblatt) is credited with the concept of [Perceptrons](http://en.wikipedia.org/wiki/Frank_Rosenblatt#Perceptron), “a machine which senses, recognizes, remembers, and responds like the human mind” as early as in 1957 but in a critical book written in 1969 by Marvin Minsky and [Seymour Papert](http://en.wikipedia.org/wiki/Seymour_Papert) showed that Rosenblatt’s original system was painfully limited, literally blind to some simple logical functions like [XOR](http://en.wikipedia.org/wiki/Exclusive_or). In the book they said: "... our intuitive judgment that the extension (to multilayer systems) is sterile". This intuition was incorrect and the field of “Neural Networks” pretty much disappeared! [Geoff Hinton](http://www.cs.toronto.edu/~hinton/) built more complex networks of virtual neurons that allowed a new generation of networks to learn more complicated functions (like the exclusive-or that had bedeviled the original Perceptron). Even the new models had serious problems though. They learned slowly and inefficiently and couldn’t master even some of the basic things that children do. By the late 1990s, neural networks had again begun to fall out of favor. In 2006, Hinton developed a new technique that he dubbed deep learning, which extends earlier important work by [Yann LeCun](http://yann.lecun.com/). Deep learning’s important innovation is to have models learn categories incrementally, attempting to nail down lower-level categories (like letters) before attempting to acquire higher-level categories (like words).

In April 2000, in a seminal work published in [Nature](http://www.nature.com/nature/journal/v404/n6780/full/404871a0.html) by [Mriganka Sur](http://surlab.mit.edu/), et. al, at [MIT's laboratory for brain and cognitive sciences](http://bcs.mit.edu/), the authors were able to successfully “rewire" brains in very young mammals, inputs from the eye were directed to brain structures that normally process hearing. The animal's auditory cortex successfully interpreted input from its eyes. But it didn't do the job as well as the primary visual cortex would have, suggesting that while the brain's plasticity, or ability to adapt, is enormous, it is limited by genetic preprogramming. Environmental input, while key to the development of brain function, does not "write on a blank slate". This addresses an age-old question – is the brain is genetically programmed or shaped by environment? It is a dramatic evidence of the ability of the developing brain to adapt to changes in the external environment, and speaks to the enormous potential and plasticity of the cerebral cortex – the seat of our highest abilities. This provided some theoretical underpinning to the neural net computation theory.

Deep neural nets spawned a subset known as Recurrent Neural Nets which were an attempt to model sequential events. [Support Vector Machines](http://www.youtube.com/watch?v=eHsErlPJWUU), [logistic regression](http://en.wikipedia.org/wiki/Logistic_regression), [feedforward networks](http://en.wikipedia.org/wiki/Feedforward_neural_network) have proved very useful without explicitly modeling time. But the assumption of independence precludes modeling long range dependencies. DNNs were also helped by the emergence of [GPU](http://www.nvidia.com/object/what-is-gpu-computing.html)s which enabled parallelism as much of computation in DNN is intrinsically parallel in nature. RNNs are connectionist models with the ability to selectively pass information across sequence steps while processing sequential data one element at a time. They can model input and/or output consisting of sequences of elements that are not independent. However, learning with recurrent networks is difficult. For standard feedforward networks, the optimization task is [NP-complete](http://www.youtube.com/watch?v=YX40hbAHx3s). Learning with recurrent networks is challenging due to the difficulty of learning long-range dependencies. Problems of vanishing and exploding gradients occur when back propagating errors across many time steps. In 1997, Hochreiter and Schmidhuber introduced the [Long Short Term Memory (LSTM)](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) model to overcome vanishing gradients. LSTMs have been proven to be remarkable in speech and handwriting recognition. Similarly, another variation of the Deep Net Model is the [Convolution Neural Network (CNN)](http://en.wikipedia.org/wiki/Convolutional_neural_network) that has been very successful in classifying images. 

In conclusion, we have come a long way. Deep Nets appear to be very promising in some areas although they are computationally very expensive. However, deep learning is only part of the larger challenge of building intelligent machines. It lacks ways of representing causal relationships, have no obvious ways of performing logical inferences, and is still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning. 

The name "Machine Learning" is indicative of the potentials that it can possibly achieve in the future. In the next article, I will talk about what problems in the industry that can be solved with the current state of technology and its evolution in the next couple of years.

